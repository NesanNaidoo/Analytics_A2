---
output:
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r, echo=FALSE, include=FALSE}
dat =read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE)
# dim(dat)
# head(dat) # First five observationsare all of class 1
# dim(x) = p = 360
# dim(y) = 

library(ggplot2)

```

# Question 1

## a)

```{r, fig.cap="Scatter Plot of coordinates by Particle Type",fig.align='center'}
# Create column for particle type based on Y1, Y2, Y3
dat$Particle <- ifelse(dat$Y1 == 1, "alpha", ifelse(dat$Y2 == 1, "beta", "rho"))

# Create the scatter plot
ggplot(data = dat, aes(x = X1, y = X2, color = Particle)) +
  geom_point(shape = 16, size = 3) +
  scale_color_manual(values = c("alpha" = "red", "beta" = "lightskyblue", "rho" = "green"),labels = c("alpha", "beta", "rho")) +  
  labs(x = "X1", y = "X2", title = "") +  
  theme_minimal() +  
  theme(legend.position = "right",axis.line = element_line(color = "black", linewidth = 1))  +
  coord_fixed()  # Ensures 1:1 aspect ratio
```

The rho particles (green) form a circular region around the origin. The
alpha particles (red) form a concave shape along the right side of the
rho and beta particles. The beta particles (blue) also form a concave
shape along the left side of the rho and alpha particles. There are
signs of overlap with all three particles

A neural network is an appropriate model class for this problem as the
particles above are not linearly separable and neural networks are good
at learning non-linear decision boundaries and capturing complex
patterns in the data.

## b)

```{r}

soft_max <- function(Z) {
  exp_Z <- exp(Z)
  denom <- colSums(exp_Z)
  probs <- exp_Z / matrix(denom, nrow = nrow(Z), ncol = ncol(Z), byrow = TRUE)
  return(probs)
}

```

## c)

```{r,eval=FALSE}
cross_ent <- function(y, y_hat) {
  if (y[1] == 1) {
    return(-log(y_hat[1]))
  } else if (y[2] == 1) {
    return(-log(y_hat[2]))
  } else if (y[3] == 1) {
    return(-log(y_hat[3]))
  }
}

```

Since $y_i$ is one-hot encoded, the terms where $y_{ij} = 0$ contribute
0 to $C_i$. Therefore, it is computationally more efficient to evaluate
only the terms corresponding to $y_{i.}=1$ as computing terms where
$y_{i.}=0$ is computationally a waste of time and resources.

## d)

```{r}
g <- function(Yhat, Y) {
  #Yhat: 3 x N matrix of predicted probabilities
  #Y: 3 x N matrix of one-hot encoded responses
  
  log_Yhat <- -log(Yhat)
  
  C_i <- colSums(Y * log_Yhat) 
  
  return(mean(C_i))
}

```

## e)

For an ($m,m$)-AFnetwork with dim(x)=$p$ , dim(y) = $q$ ,

-   Augmented features: original $p$ inputs are transformed into $p$
    augmented features via a **s**eparate hidden layer of size $p$.

<!-- -->

-   $2p$ input nodes = (Original $p$ nodes + Augmented features $p$ ).

-   $m$ nodes in the first hidden layer.

-   $m$ nodes in the second hidden layer.

-   $q$ output nodes.

-   Augmentation Layer: transforms original $p$ inputs into $p$
    augmented features.

    -   $p^2$ Weights + $p$ Bias terms = $p^2 +p$ parameters.

-   Input to First Hidden Layer:

    -   $2pm$ Weights+ $m$ Bias terms = $2pm +m$ parameters.

-   First to Second Hidden Layer:

    -   $m^2$ Weights + $m$ Bias Terms $= m^2 + m$ parameters.

-   Second Hidden to Output Layer:

    -   $mq$ weights + $q$ biases $= mq+ q$ parameters.

-   Total parameters $= p^2+p+2pmm^2+2m +mq+q$.

-   Total parameters $= p(p + 1+2m) +m (m +2 +q) +q$.

## f) 

```{r}

# activation function for the augmented and hidden layers 
activation_fn <- function(Z){
  tanh(Z)
}

# Write a function that evaluates the neural network (forward recursion):
# X     - Input matrix (N x p)
# Y     - Output matrix(N x q)
# theta - A parameter vector (all of the parameters)
# m     - Number of nodes on hidden layers
# v     - regularisation parameter
forward_pass <- function(X, Y, theta, m, v){
  
  # Relevant dimensional variables:
   N = dim(X)[1]
   p = dim(X)[2]
   q = dim(Y)[2]
   
   # Populate weight-matrix and bias vectors:
   index = 1:(p*p)
   W1    = matrix(theta[index], p, p) # augmented layer
   index = max(index)+1:(2*p*m)
   W2    = matrix(theta[index], 2*p, m) # hidden layer 1
   index = max(index)+1:(m*m)
   W3    = matrix(theta[index], m, m) # hidden layer 2
   index = max(index)+1:(m*q)
   W4    = matrix(theta[index], m, q) # output layer
   
   index = max(index) + 1:(p)
   b1    = matrix(theta[index],p , 1) # augmented layer
   index = max(index)+1:(m)
   b2    = matrix(theta[index],m,1)   # hidden layer 1
   index = max(index)+1:(m)
   b3    = matrix(theta[index],m,1)   # hidden layer 2
   index = max(index)+1:(q)
   b4    = matrix(theta[index],q,1)   # output layer 
  
   # calculate the activation functions
   ones_t = matrix(1,1,N)
   A0 = t(X)
   A1 = activation_fn(t(W1) %*% A0 +b1 %*% ones_t) # pxN
   A_aug = rbind(A0, A1)                           # 2pxN
   A2 = activation_fn(t(W2) %*% A_aug +b2 %*% ones_t) # mxN
   A3 = activation_fn(t(W3) %*% A2 +b3 %*% ones_t) # mxN
   A4 = soft_max(t(W4) %*% A3 + b4 %*% ones_t)     # qnN
   
   # calculate the errors
   Yhat = t(A4)
   E1 = g(t(Yhat), t(Y)) # error 1- cross entropy
   E2 = E1 + v/N * (sum(W1^2) + sum(W2^2) + sum(W3^2) + sum(W4^2))
   
   return(list(out = Yhat, E1 = E1, E2 = E2))
  
}
```

## g) 

```{r, fig.cap= "Showing Validation Error vs v of the neural network"}
set.seed(2025)

# extract input and output observations
X <-  as.matrix(dat[,1:3])
Y <-  as.matrix(dat[,4:6])

N <-  dim(X)[1]

# split the data into 80-20 ratio for training & validation
set <- sample(1:N, round(0.8 * N), replace = FALSE)
X_train <- X[set,]
Y_train <- Y[set,]
X_val <- X[-set,]
Y_val <- Y[-set,]

# perform neural network using feed forward function
p <- dim(X)[2]
q <- dim(Y)[2]
m <- 4
npars <- p*(p + 1 + 2*m) + m*(m + 2 + q) + q

nv <- 20
v_seq <-  exp(seq(-6,2,length.out = nv))
validation_errors <- rep(NA, nv)


# validation
for(i in 1:nv){
  v <- v_seq[i]
  
  # penalised obj fn
  obj_pen = function(pars)
  {
    res_model = forward_pass(X_train, Y_train, pars, m, v) #NB training data
    return(res_model$E2)
  }
  
  theta_rand <- runif(npars,-1,1)
  res_opt = nlm(obj_pen, theta_rand, iterlim = 2000)
  
  res_fitted = forward_pass(X_val, Y_val, res_opt$estimate, m, 0)
  validation_errors[i] <- res_fitted$E1
  
}

# select optimal v
v_opt <- v_seq[which.min(validation_errors)]

plot_data <- data.frame(v = v_seq, ValidationError = validation_errors)
ggplot(plot_data, aes(x = v, y = ValidationError)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  geom_vline(xintercept = v_opt, linetype = "dotted", color = "red", size = 1) +
  labs(x = "Î½", y = "Validation Cross-Entropy Error", title = "") +
  xlim(0, 2.5) +
  theme_minimal() +
  coord_fixed()


```

We can see from the red dotted line, that the minimum validation cross
entropy error occurs at the reularisation level of `r round(v_opt, 3)`. Therefore we will choose this value as our regularisation level. 

## h) 

```{r, fig.cap="Response curves for each variable by their detector type"}
# train final model with optimal v_opt
obj_pen_final = function(pars) {
  forward_pass(X_train, Y_train, pars, m, v_opt)$E2
}

theta_rand_final <- runif(npars, -1, 1)
res_opt_final <- nlm(obj_pen_final, theta_rand_final, iterlim = 2000)

# function to create response curves
create_response_curves <- function(model, X_data, var_index, n_points = 100) {
  # Create sequence for the selected variable
  var_seq <- seq(min(X_data[,var_index]), max(X_data[,var_index]), length.out = n_points)
  
  # Create reference data (median values for other variables)
  ref_data <- apply(X_data, 2, median)
  response_data <- matrix(rep(ref_data, each = n_points), nrow = n_points)
  response_data[,var_index] <- var_seq
  
  # Get predictions
  preds <- forward_pass(response_data, matrix(0, n_points, q), 
                       res_opt_final$estimate, m, 0)$out
  
  # Prepare data for plotting
  plot_data <- data.frame(
    x = rep(var_seq, q),
    y = as.vector(preds),
    Detector = factor(rep(c("Y1", "Y2", "Y3"), each = n_points)),
    Variable = colnames(X_data)[var_index]
  )
  
  return(plot_data)
}

# Create response curves for each input variable
response_curves <- rbind(
  create_response_curves(res_opt_final, X_train, 1),  # X1
  create_response_curves(res_opt_final, X_train, 2),  # X2
  create_response_curves(res_opt_final, X_train, 3)   # X3
)

# Plot the response curves
ggplot(response_curves, aes(x = x, y = y, color = Detector)) +
  geom_line(size = 1) +
  facet_wrap(~Variable, scales = "free_x") +
  labs(x = "Input Value", y = "Predicted Probability", color = "Detector") +
  scale_color_manual(values = c("Y1" = "red", "Y2" = "blue", "Y3" = "green")) +
  theme_minimal() +
  theme(legend.position = "bottom")


```

## i)(Nesan)

AFnetworks introduce a dedicated hidden layer that learns nonlinear
patterns of the input features (e.g. rho particles in circular regions and alpha and beta particles in concave shapes) before they are fed into the main
network. These transformations act like automatic, learned features that
might capture complex patterns the original inputs could not express on
their own leading to better generalization and performance of the model.

Addtionally, as AFnetworks explicitly separate the original inputs from
augmented inputs. This allows us to understand what features matter and
how they matter. Hence, helps us understand how the network is reshaping
the data before making predictions. For example if we needed to take a closer look at the results it produces, we could look at the value produced by the activation function of tanh in A1 for different observations. 

The AFnetworks, augmentation layer performs feature engineering which reduces the need for large hidden layers to correctly classify observation. For example in our AFnetwork, we only needed 4 nodes in our hidden layers instead of needing more nodes/layers which could have become computationally expensive without our augmented layer. 


